# ğŸ€ AnÃ¡lisis de Datos NBA â€“ Proyecto Final | Equipo 1

Este repositorio contiene el desarrollo de nuestro proyecto grupal, centrado en el anÃ¡lisis y visualizaciÃ³n de datos de la NBA. El objetivo principal fue extraer valor de distintas fuentes de datos relacionadas al rendimiento de equipos y jugadores para construir visualizaciones interactivas que faciliten la toma de decisiones.

---


## ğŸ“¥ Descarga de Datos Crudos

Debido a limitaciones de tamaÃ±o, los datos originales no estÃ¡n almacenados directamente en este repositorio. En su lugar, podÃ©s descargar los datos crudos desde la carpeta compartida en Google Drive:

**Nota:** Es importante colocar los datos descargados dentro de la carpeta `Data/Data Cruda` para que los notebooks funcionen correctamente con las rutas relativas configuradas.

[ğŸ“‚ Carpeta de Google Drive][![Google Drive](https://img.shields.io/badge/Google%20Drive-Download-blue?logo=google-drive&style=flat-square)](https://drive.google.com/drive/folders/1cpyqh4gJj8WTLE0hibV6B5b3XWL0TMEG)

## ğŸ“ Estructura del Repositorio

```
PF_NBA_EQUIPO1/
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ Data Cruda/       # Contiene un link a Drive para descargar los datos originales
â”‚   â””â”€â”€ Data Limpia/           # Datos limpios y transformados listos para anÃ¡lisis
â”‚
â”œâ”€â”€ Notebooks/            # Notebooks individuales de cada integrante (limpieza y transformaciÃ³n)
â”‚
â”œâ”€â”€ DocumentaciÃ³n/
â”‚   â”œâ”€â”€ Identidad/        # Elementos visuales del proyecto
â”‚   â”œâ”€â”€ diccionario_datos.pdf
â”‚   â””â”€â”€ informe_proyecto.pdf
â”‚
â”œâ”€â”€ requirements.txt      # Dependencias del entorno
â””â”€â”€ README.md             # DocumentaciÃ³n principal del proyecto
```

---

## ğŸ¯ Objetivos del Proyecto

- Recolectar, limpiar y transformar datasets relacionados a la NBA.
- Desarrollar un modelo de procesamiento de datos reproducible.
- Subir los datos procesados a Google Cloud Platform.
- Crear dashboards interactivos en Power BI.
- Documentar el flujo completo de trabajo para futuras implementaciones.

---

## âš™ï¸ InstalaciÃ³n del Entorno de Trabajo

1. Clonar el repositorio:

   ```bash
   git clone https://github.com/pazcaminoDA/PF_NBA_EQUIPO1.git
   cd PF_NBA_EQUIPO1
   ```

2. (Opcional) Crear un entorno virtual:

   ```bash
   python -m venv env
   source env/bin/activate   # Linux/MacOS
   .\env\Scripts\activate    # Windows
   ```

3. Instalar las dependencias:

   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ”„ Flujo de Trabajo

## ğŸš€ Instructivo para encarar el proyecto

A continuaciÃ³n se detallan los pasos recomendados para trabajar con este proyecto de forma ordenada y eficiente:

1. **Descargar los datos crudos**  
   DescargÃ¡ los datasets originales desde la carpeta de Google Drive y colocÃ¡ los archivos dentro de `Data/Data Cruda` (carpetas creadas localmente al clonar el repositorio).

2. **Preparar el entorno de trabajo**  
   AbrÃ­ Visual Studio Code y cargÃ¡ la carpeta del proyecto para trabajar con los notebooks.  
   Instalar las dependencias listadas en `requirements.txt`.

3. **Limpieza y transformaciÃ³n de datos**  
   EjecutÃ¡ y modificÃ¡ los notebooks en la carpeta `Notebooks` para procesar los datos crudos y obtener datasets limpios.  
   Los archivos transformados deben guardarse en la carpeta `Data/Data limpia`.

4. **Subida a Google Cloud Platform**  
   Los datasets limpios se cargan a Google Cloud Storage o BigQuery para centralizar la informaciÃ³n y facilitar la integraciÃ³n con herramientas de visualizaciÃ³n.

5. **VisualizaciÃ³n en Power BI**  
   ConectÃ¡ Power BI a las fuentes de datos en la nube para crear dashboards dinÃ¡micos e interactivos que permitan analizar la informaciÃ³n de manera efectiva.

6. **AutomatizaciÃ³n del pipeline**  
   Actualmente se estÃ¡ trabajando en la automatizaciÃ³n de los procesos de extracciÃ³n, transformaciÃ³n y carga (ETL) para optimizar y agilizar las actualizaciones periÃ³dicas de datos.  
   Esto incluye la programaciÃ³n de scripts y la integraciÃ³n con herramientas como cron, Airflow o Cloud Functions.


---

## ğŸ“ˆ Herramientas y TecnologÃ­as Utilizadas

- **Python**: pandas, numpy, seaborn, matplotlib  
- **Jupyter Notebooks**  
- **Google Cloud Platform**: Cloud Storage, BigQuery  
- **Power BI**  
- **Visual Studio Code**  
- **Git & GitHub** para control de versiones

---

## ğŸ¤– AutomatizaciÃ³n del Pipeline (En Desarrollo)

Estamos trabajando en la implementaciÃ³n de un pipeline automÃ¡tico para:

- Extraer, transformar y cargar los datos con scripts programados.
- Automatizar la carga de datos limpios a GCP (Storage o BigQuery).
- Ejecutar procesos mediante `cron`, Airflow o Cloud Functions.
- Garantizar trazabilidad y repetibilidad del proceso de ETL.

---

## ğŸ“Š Visualizaciones en Power BI

El producto final incluye dashboards conectados directamente a la nube. Estas visualizaciones permiten:

- Monitorear KPIs relevantes
- Acceso dinÃ¡mico a la informaciÃ³n
- Toma de decisiones basada en datos

---

## ğŸ‘¥ Integrantes del Equipo

- SofÃ­a Echeverria  
- Leonel Fuhrmann  
- Maria Paz Camino  
- AgustÃ­n Brandt


