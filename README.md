# ğŸ€ AnÃ¡lisis de Datos NBA â€“ Proyecto Final | Equipo 1

Este repositorio contiene el desarrollo de nuestro proyecto grupal, centrado en el anÃ¡lisis y visualizaciÃ³n de datos de la NBA. El objetivo principal fue extraer valor de distintas fuentes de datos relacionadas al rendimiento de equipos y jugadores para construir visualizaciones interactivas que faciliten la toma de decisiones.

---

## ğŸ“ Estructura del Repositorio

PF_NBA_EQUIPO1/
â”‚
â”œâ”€â”€ Data/
â”‚ â”œâ”€â”€ Data Cruda/ # Contiene un link de una carpeta de drive donde se descargan los archivos crudos
â”‚ â””â”€â”€ limpia/ # Datos limpios y transformados listos para anÃ¡lisis
â”‚
â”œâ”€â”€ Notebooks/ # Notebooks que contienen las transformaciones hechas por cada uno de los integrantes, en carpetas separadas 
â”‚
â”‚
â”œâ”€â”€ DocumentaciÃ³n/
â”‚ â”œâ”€â”€ Identidad/ # Elementos visuales e identidad del proyecto
â”‚ â”œâ”€â”€ diccionario_datos.pdf
â”‚ â””â”€â”€ informe_proyecto.pdf
â”‚
â”œâ”€â”€ requirements.txt # Lista de dependencias del entorno
â””â”€â”€ README.md # DocumentaciÃ³n principal del proyecto

---

## ğŸ¯ Objetivos del Proyecto

- Recolectar, limpiar y transformar datasets relacionados a la NBA.
- Desarrollar un modelo de procesamiento de datos reproducible.
- Subir los datos procesados a la nube mediante Google Cloud Platform.
- Crear dashboards interactivos en Power BI para facilitar el anÃ¡lisis exploratorio y descriptivo.
- Documentar el flujo completo de trabajo para futuras implementaciones.

---

## âš™ï¸ InstalaciÃ³n del Entorno de Trabajo

1. Clonar el repositorio:

   git clone https://github.com/pazcaminoDA/PF_NBA_EQUIPO1.git
   cd PF_NBA_EQUIPO1

Crear entorno virtual (opcional):

python -m venv env
source env/bin/activate   # Linux/MacOS
.\env\Scripts\activate    # Windows

Instalar las dependencias:
pip install -r requirements.txt


ğŸ”„ Flujo de Trabajo
ObtenciÃ³n de datos: los datasets fueron descargados desde distintas fuentes y almacenados en Drive.

DivisiÃ³n de tareas: cada integrante trabajÃ³ en la limpieza y transformaciÃ³n de datasets especÃ­ficos utilizando Jupyter Notebooks.

ConsolidaciÃ³n: los archivos limpios fueron centralizados y subidos a una instancia de almacenamiento en Google Cloud Platform (GCP).

VisualizaciÃ³n: conexiÃ³n de los datos en GCP con Power BI para desarrollar visualizaciones interactivas.

ğŸ“ˆ Herramientas y TecnologÃ­as Utilizadas
Python (pandas, numpy, seaborn, matplotlib)

Jupyter Notebooks

Google Cloud Platform (Cloud Storage, BigQuery)

Power BI

Git & GitHub para control de versiones

Visual Studio Code

ğŸ¤– AutomatizaciÃ³n de Ingesta de Datos (En Desarrollo)
Actualmente se encuentra en desarrollo una soluciÃ³n de automatizaciÃ³n del pipeline de ingesta y procesamiento de datos. El objetivo es:

Implementar scripts programados para extraer y transformar los datasets originales.

Automatizar la carga de datos limpios a Google Cloud Storage o BigQuery.

Utilizar herramientas como cron, Airflow o Cloud Functions para ejecutar procesos en intervalos definidos.

Garantizar la trazabilidad y repetibilidad del proceso de ETL.

ğŸ“Š Visualizaciones en Power BI

El producto final incluye dashboards conectados directamente a los datos en la nube, permitiendo una visualizaciÃ³n dinÃ¡mica, actualizada y centrada en KPIs relevantes.



ğŸ‘¥ Integrantes del Equipo
SofÃ­a Echeverria 

Leonel Fuhrmann

Maria Paz Camino

AgustÃ­n Brandt

