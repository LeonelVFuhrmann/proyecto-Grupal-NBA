# ğŸ€ AnÃ¡lisis de Datos NBA â€“ Proyecto Final | Equipo 1

Este repositorio contiene el desarrollo de nuestro proyecto grupal, centrado en el anÃ¡lisis y visualizaciÃ³n de datos de la NBA. El objetivo principal fue extraer valor de distintas fuentes de datos relacionadas al rendimiento de equipos y jugadores para construir visualizaciones interactivas que faciliten la toma de decisiones.

---

## ğŸ“ Estructura del Repositorio

```
PF_NBA_EQUIPO1/
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ Data Cruda/       # Contiene un link a Drive para descargar los datos originales
â”‚   â””â”€â”€ limpia/           # Datos limpios y transformados listos para anÃ¡lisis
â”‚
â”œâ”€â”€ Notebooks/            # Notebooks individuales de cada integrante (limpieza y transformaciÃ³n)
â”‚
â”œâ”€â”€ DocumentaciÃ³n/
â”‚   â”œâ”€â”€ Identidad/        # Elementos visuales del proyecto
â”‚   â”œâ”€â”€ diccionario_datos.pdf
â”‚   â””â”€â”€ informe_proyecto.pdf
â”‚
â”œâ”€â”€ requirements.txt      # Dependencias del entorno
â””â”€â”€ README.md             # DocumentaciÃ³n principal del proyecto
```

---

## ğŸ¯ Objetivos del Proyecto

- Recolectar, limpiar y transformar datasets relacionados a la NBA.
- Desarrollar un modelo de procesamiento de datos reproducible.
- Subir los datos procesados a Google Cloud Platform.
- Crear dashboards interactivos en Power BI.
- Documentar el flujo completo de trabajo para futuras implementaciones.

---

## âš™ï¸ InstalaciÃ³n del Entorno de Trabajo

1. Clonar el repositorio:

   ```bash
   git clone https://github.com/pazcaminoDA/PF_NBA_EQUIPO1.git
   cd PF_NBA_EQUIPO1
   ```

2. (Opcional) Crear un entorno virtual:

   ```bash
   python -m venv env
   source env/bin/activate   # Linux/MacOS
   .\env\Scripts\activate    # Windows
   ```

3. Instalar las dependencias:

   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ”„ Flujo de Trabajo

1. **ObtenciÃ³n de Datos:**  
   Los datasets fueron descargados desde distintas fuentes y almacenados en una carpeta compartida de Google Drive.

2. **Limpieza y TransformaciÃ³n:**  
   Cada integrante trabajÃ³ en distintos datasets utilizando Jupyter Notebooks.

3. **ConsolidaciÃ³n:**  
   Los archivos limpios se centralizaron y subieron a Google Cloud Platform.

4. **VisualizaciÃ³n:**  
   Se conectaron los datos desde GCP a Power BI para construir dashboards interactivos.

---

## ğŸ“ˆ Herramientas y TecnologÃ­as Utilizadas

- **Python**: pandas, numpy, seaborn, matplotlib  
- **Jupyter Notebooks**  
- **Google Cloud Platform**: Cloud Storage, BigQuery  
- **Power BI**  
- **Visual Studio Code**  
- **Git & GitHub** para control de versiones

---

## ğŸ¤– AutomatizaciÃ³n del Pipeline (En Desarrollo)

Estamos trabajando en la implementaciÃ³n de un pipeline automÃ¡tico para:

- Extraer, transformar y cargar los datos con scripts programados.
- Automatizar la carga de datos limpios a GCP (Storage o BigQuery).
- Ejecutar procesos mediante `cron`, Airflow o Cloud Functions.
- Garantizar trazabilidad y repetibilidad del proceso de ETL.

---

## ğŸ“Š Visualizaciones en Power BI

El producto final incluye dashboards conectados directamente a la nube. Estas visualizaciones permiten:

- Monitorear KPIs relevantes
- Acceso dinÃ¡mico a la informaciÃ³n
- Toma de decisiones basada en datos

---

## ğŸ‘¥ Integrantes del Equipo

- SofÃ­a Echeverria  
- Leonel Fuhrmann  
- Maria Paz Camino  
- AgustÃ­n Brandt


